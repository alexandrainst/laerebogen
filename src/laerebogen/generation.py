"""Generation of the instruction-following data using a language model."""

import json
import logging
import random
import re
import string
import time
import typing as t
from functools import partial
from multiprocessing import Pool
from pathlib import Path

import numpy as np
import tqdm
from rouge_score import rouge_scorer

from .data_models import InstructionSample
from .ollama_utils import generate_text_with_ollama, try_download_ollama_model

if t.TYPE_CHECKING:
    import ollama


logger = logging.getLogger(__name__)


def generate_instruction_following_data(
    output_dir: str,
    seed_tasks_path: str,
    num_instructions_to_generate: int,
    model_name: str,
    num_prompt_instructions: int,
    request_batch_size: int,
    num_cpus: int,
) -> None:
    """Generate instructions following the seed tasks.

    This function loads seed tasks, loads generated tasks (regen.json if it exists) and
    generates a batch of new instructions using the OpenAI API.

    The new batch is generated by sampling num_prompt_instructions tasks from the seed
    tasks and using them as a prompt for the LLM. The model is asked to generate a new
    instruction task based on the prompt. The generated tasks are filtered based on
    their similarity to the existing tasks and saved to a JSON file (regen.json).

    Args:
        output_dir:
            Directory to save the generated dataset.
        seed_tasks_path:
            Path to the seed tasks file.
        num_instructions_to_generate:
            Number of instructions to generate.
        model_name:
            The Ollama model ID of the model to use for generation. Must be a base
            model, not a finetuned one.
        num_prompt_instructions:
            Number of instructions to use as prompts for each generated instruction.
        request_batch_size:
            Number of requests to send to the model at once.
        num_cpus:
            Number of CPUs to use for parallel processing.
    """
    # Download the model, if it hasn't been downloaded yet
    try_download_ollama_model(model_id=model_name)

    # Load the seed tasks
    with Path(seed_tasks_path).open() as f:
        seed_tasks = [json.loads(line) for line in f.readlines() if line.strip()]
    seed_instruction_data = [
        InstructionSample(
            instruction=t["instruction"],
            input=t["instances"][0]["input"],
            output=t["instances"][0]["output"],
        )
        for t in seed_tasks
    ]
    logger.info(f"Loaded {len(seed_instruction_data)} human-written seed instructions")

    # Load the LM-generated instructions
    machine_instruction_path = Path(output_dir, "regen.json")
    machine_instruction_path.parent.mkdir(parents=True, exist_ok=True)
    machine_instruction_data = []
    if machine_instruction_path.exists():
        with machine_instruction_path.open() as f:
            machine_instruction_data = json.load(f)
        logger.info(
            f"Loaded {len(machine_instruction_data):,} machine-generated instructions."
        )

    # Initialise the Rouge scorer for similarity scoring
    scorer = rouge_scorer.RougeScorer(["rougeL"], use_stemmer=False)

    # Initialise the progress bar
    progress_bar = tqdm.tqdm(total=num_instructions_to_generate)
    if machine_instruction_data:
        progress_bar.update(len(machine_instruction_data))

    # First we tokenise all the seed instructions and generated machine instructions
    all_instructions = [
        seed_instruction.instruction for seed_instruction in seed_instruction_data
    ] + [instruction["instruction"] for instruction in machine_instruction_data]
    all_instruction_tokens = [
        scorer._tokenizer.tokenize(inst) for inst in all_instructions
    ]

    # Start generating instructions
    request_idx = 0
    while len(machine_instruction_data) < num_instructions_to_generate:
        request_idx += 1

        # Randomly sample some seed instructions, that the new instructions should be
        # based on
        batch_inputs = []
        for _ in range(request_batch_size):
            seed_instructions = random.sample(
                seed_instruction_data, num_prompt_instructions
            )
            prompt = encode_prompt(seed_instructions=seed_instructions)
            batch_inputs.append(prompt)

        # Generate new instructions with the LLM
        request_start = time.time()
        responses = generate_text_with_ollama(
            prompts=batch_inputs, model_name=model_name, batch_size=request_batch_size
        )
        request_duration = time.time() - request_start

        # Process the generated instructions
        process_start = time.time()
        instruction_data: list[InstructionSample] = []
        for response in responses:
            new_instructions = post_process_response(
                num_prompt_instructions=num_prompt_instructions, response=response
            )
            instruction_data += new_instructions

        # Filter the generated instructions to not keep too similar ones
        total = len(instruction_data)
        keep = 0
        for instruction_data_entry in instruction_data:
            # Compute the similarity of the new instruction to all existing
            # instructions
            new_instruction_tokens = scorer._tokenizer.tokenize(
                instruction_data_entry.instruction
            )
            with Pool(num_cpus) as p:
                rouge_scores = p.map(
                    partial(rouge_scorer._score_lcs, new_instruction_tokens),
                    all_instruction_tokens,
                )
            rouge_scores = [score.fmeasure for score in rouge_scores]
            most_similar_instructions = {
                all_instructions[i]: rouge_scores[i]
                for i in np.argsort(rouge_scores)[-10:][::-1]
            }

            # If the new instruction is too similar to existing ones, we skip it
            if max(rouge_scores) > 0.7:
                continue
            keep += 1

            # Store the similarity data in the instruction data entry
            instruction_data_entry.most_similar_instructions = most_similar_instructions
            instruction_data_entry.avg_similarity_score = float(np.mean(rouge_scores))

            # Store the generated data
            machine_instruction_data.append(instruction_data_entry)
            all_instructions.append(instruction_data_entry.instruction)
            all_instruction_tokens.append(new_instruction_tokens)

            # Update the progress bar
            progress_bar.update(1)

        # Stop the process timer and log the durations
        process_duration = time.time() - process_start
        logger.info(
            f"Request {request_idx} took {request_duration:.2f}s, processing "
            f"took {process_duration:.2f}s"
        )
        logger.info(f"Generated {total:,} instructions, kept {keep:,} instructions")

        # Store the generated instructions to disk
        with Path(output_dir, "regen.json").open("w") as f:
            json.dump(machine_instruction_data, f, indent=4, ensure_ascii=False)

    # Close the progress bar
    progress_bar.close()


def encode_prompt(seed_instructions: list[InstructionSample]) -> str:
    """Encode multiple prompt instructions into a single string.

    Args:
        seed_instructions:
            A list of seed instructions to encode into the prompt.

    Returns:
        A string containing the encoded prompt.
    """
    with Path("data/prompt.txt").open() as f:
        prompt = f.read() + "\n"
    idx = 0
    for idx, seed in enumerate(seed_instructions, start=1):
        instruction = re.sub(r"\s+", " ", seed.instruction).strip().rstrip(":")
        input = "<noinput>" if seed.input.lower() == "" else seed.input
        prompt += "###\n"
        prompt += f"{idx}. Instruktion: {instruction}\n"
        prompt += f"{idx}. Input:\n{input}\n"
        prompt += f"{idx}. Output:\n{seed.output}\n"
    prompt += f"###\n{idx + 1}. Instruktion:"
    return prompt


def post_process_response(
    num_prompt_instructions: int, response: "ollama.GenerateResponse"
) -> list[InstructionSample]:
    """Post-process the response from the model to extract instructions.

    Args:
        num_prompt_instructions:
            Number of prompt instructions used in the request.
        response:
            The response from the model.

    Returns:
        A list of seed instructions extracted from the response.
    """
    raw_instructions = (
        f"{num_prompt_instructions + 1}. Instruktion:" + response.response
    )
    raw_instructions = re.split("###", raw_instructions)
    instructions = []

    for idx, inst in enumerate(raw_instructions):
        # if the decoding stops due to length, the last example is likely truncated so
        # we discard it
        if idx == len(raw_instructions) - 1 and response.done_reason == "length":
            continue

        idx += num_prompt_instructions + 1

        # If the data does not contain the expected format, we skip it
        splitted_data = re.split(rf"{idx}\.\s+(Instruktion|Input|Output):", inst)
        if len(splitted_data) != 7:
            continue

        inst = splitted_data[2].strip()
        input = splitted_data[4].strip()
        input = "" if input.lower() == "<noinput>" else input
        output = splitted_data[6].strip()

        # If the instruction is too short or too long, we skip it
        if len(inst.split()) <= 3 or len(inst.split()) > 150:
            continue

        # Filter based on keywords that are not suitable for language models
        blacklist = [
            "image",
            "images",
            "graph",
            "graphs",
            "picture",
            "pictures",
            "file",
            "files",
            "map",
            "maps",
            "draw",
            "plot",
            "go to",
            "video",
            "audio",
            "music",
            "flowchart",
            "diagram",
            "billede",
            "billeder",
            "graf",
            "grafer",
            "foto",
            "fotos",
            "fil",
            "filer",
            "illustrer",
            "illustration",
            "tegning",
            "gå til",
            "musik",
        ]
        blacklist += []
        if re.search(
            pattern=r"\b|\b".join([rf"\b{word}\b" for word in blacklist]),
            string=inst,
            flags=re.IGNORECASE,
        ):
            continue

        # We found that the model tends to add "write a program" to some existing
        # instructions, which lead to a lot of such instructions, and it's a bit
        # comfusing whether the model need to write a program or directly output the
        # result. Here we filter them out. Note this is not a comprehensive filtering
        # for all programming instructions.
        if inst.startswith("Skriv et program"):
            continue

        # Filter those starting with punctuation
        if inst[0] in string.punctuation:
            continue

        # Filter those starting with non-Danish character
        if not inst[0].isascii() and inst[0].lower() not in {"æ", "ø", "å"}:
            continue

        # Store the instruction
        new_instruction = InstructionSample(
            instruction=inst, input=input, output=output
        )
        instructions.append(new_instruction)

    return instructions
