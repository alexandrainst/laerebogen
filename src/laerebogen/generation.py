"""Generation of the instruction-following data using a language model.

This is the method described in [1], based on [2], and uses seed tasks to generate new
instructions using a base decoder model.

[1]: https://github.com/tatsu-lab/stanford_alpaca
[2]: https://doi.org/10.18653/v1/2023.acl-long.754
"""

import json
import logging
import random
import re
from copy import deepcopy
from functools import partial
from multiprocessing import Pool
from pathlib import Path

import numpy as np
from rouge_score import rouge_scorer
from tqdm.auto import tqdm

from .data_models import InstructionSample, Response
from .filtering import keep_instruction
from .vllm_utils import generate_text_with_vllm, load_vllm_model

logger = logging.getLogger(__name__)


def generate_instruction_following_data(
    output_dir: str,
    prompt_path: str,
    seed_tasks_path: str,
    num_instructions_to_generate: int,
    model_id: str,
    num_prompt_instructions: int,
    batch_size: int,
    num_cpus: int,
) -> None:
    """Generate instructions following the seed tasks.

    This function loads seed tasks, loads generated tasks and generates a batch of new
    instructions.

    The new batch is generated by sampling num_prompt_instructions tasks from the seed
    tasks and using them as a prompt for the LLM. The model is asked to generate a new
    instruction task based on the prompt. The generated tasks are filtered based on
    their similarity to the existing tasks and saved to a JSON file.

    Args:
        output_dir:
            Directory to save the generated dataset.
        prompt_path:
            Path to the prompt file.
        seed_tasks_path:
            Path to the seed tasks file.
        num_instructions_to_generate:
            Number of instructions to generate.
        model_id:
            The model ID of the model to use for generation. Must be a base model, not a
            finetuned one.
        num_prompt_instructions:
            Number of instructions to use as prompts for each generated instruction.
        batch_size:
            Number of requests to send to the model at once.
        num_cpus:
            Number of CPUs to use for parallel processing.
    """
    logger.info(f"Loading model {model_id!r} for generating instructions...")
    model = load_vllm_model(model_id=model_id)

    # Load the prompt
    with Path(prompt_path).open() as f:
        generation_prompt = f.read()

    # Load the seed tasks
    with Path(seed_tasks_path).open() as f:
        seed_tasks = [json.loads(line) for line in f.readlines() if line.strip()]
    seed_instruction_data = [
        InstructionSample(
            instruction=t["instruction"],
            input=t["instances"][0]["input"],
            output=t["instances"][0]["output"],
        )
        for t in seed_tasks
    ]
    logger.info(f"Loaded {len(seed_instruction_data)} human-written seed instructions.")

    # Ensure that the output file exists
    output_path = Path(output_dir, "dataset.jsonl")
    output_path.parent.mkdir(parents=True, exist_ok=True)
    output_path.touch(exist_ok=True)

    # Load the LM-generated instructions
    machine_instruction_data = []
    if output_path.exists():
        with output_path.open() as f:
            machine_instruction_data = [
                json.loads(line) for line in f.readlines() if line.strip()
            ]
        logger.info(
            f"Loaded {len(machine_instruction_data):,} machine-generated instructions."
        )

    # Initialise the Rouge scorer for similarity scoring
    scorer = rouge_scorer.RougeScorer(rouge_types=["rougeL"], use_stemmer=False)

    # Initialise the progress bar
    progress_bar = tqdm(total=num_instructions_to_generate)
    if machine_instruction_data:
        progress_bar.n = len(machine_instruction_data)

    # First we tokenise all the seed instructions and generated machine instructions
    all_instructions = [
        seed_instruction.instruction for seed_instruction in seed_instruction_data
    ] + [instruction["instruction"] for instruction in machine_instruction_data]
    all_instruction_tokens = [
        scorer._tokenizer.tokenize(text=inst) for inst in all_instructions
    ]

    # Start generating instructions
    while len(machine_instruction_data) < num_instructions_to_generate:
        # Randomly sample some seed instructions, that the new instructions should be
        # based on
        batch_inputs = []
        for _ in range(batch_size):
            seed_instructions = random.sample(
                population=seed_instruction_data, k=num_prompt_instructions
            )
            encoded_prompt = encode_prompt(
                seed_instructions=seed_instructions, prompt=generation_prompt
            )
            batch_inputs.append(encoded_prompt)

        # Generate new instructions with the LLM
        responses = generate_text_with_vllm(prompts=batch_inputs, model=model)

        # Process the generated instructions
        instruction_data: list[InstructionSample] = []
        for response in tqdm(
            iterable=responses, desc="Processing responses", leave=False
        ):
            new_instructions = post_process_response(
                num_prompt_instructions=num_prompt_instructions,
                response=response,
                scorer=scorer,
                previous_instructions=all_instructions,
                previous_instruction_tokens=all_instruction_tokens,
                num_cpus=num_cpus,
            )
            for instruction in new_instructions:
                if keep_instruction(instruction_sample=instruction):
                    instruction_data.append(instruction)

        if instruction_data:
            # Update the existing intermediate data
            machine_instruction_data.extend(instruction_data)
            all_instructions.extend(
                instruction.instruction for instruction in instruction_data
            )
            all_instruction_tokens.extend(
                instruction.instruction_tokens for instruction in instruction_data
            )

            # Update the progress bar
            progress_bar.update(len(instruction_data))

            # Store the generated instructions to disk
            with output_path.open("a") as f:
                json_records = "\n".join(
                    instruction.json() for instruction in instruction_data
                )
                f.write(json_records + "\n")

    # Close the progress bar
    progress_bar.close()


def encode_prompt(seed_instructions: list[InstructionSample], prompt: str) -> str:
    """Encode multiple prompt instructions into a single string.

    Args:
        seed_instructions:
            A list of seed instructions to encode into the prompt.
        prompt:
            The initial prompt text to which the seed instructions will be appended.

    Returns:
        A string containing the encoded prompt.
    """
    encoded_prompt = deepcopy(prompt) + "\n"
    idx = 0
    for idx, seed in enumerate(seed_instructions, start=1):
        instruction = re.sub(r"\s+", " ", seed.instruction).strip().rstrip(":")
        input = "<noinput>" if seed.input.lower() == "" else seed.input
        encoded_prompt += "###\n"
        encoded_prompt += f"{idx}. Instruktion:\n{instruction}\n"
        encoded_prompt += f"{idx}. Input:\n{input}\n"
        encoded_prompt += f"{idx}. Output:\n{seed.output}\n"
    encoded_prompt += f"###\n{idx + 1}. Instruktion:\n"
    return encoded_prompt


def post_process_response(
    num_prompt_instructions: int,
    response: Response,
    scorer: rouge_scorer.RougeScorer,
    previous_instructions: list[str],
    previous_instruction_tokens: list[list[str]],
    num_cpus: int,
) -> list[InstructionSample]:
    """Post-process the response from the model to extract instructions.

    Args:
        num_prompt_instructions:
            Number of prompt instructions used in the request.
        response:
            The response from the model.
        scorer:
            The Rouge scorer used to compute similarity scores.
        previous_instructions:
            A list of all previously generated instructions.
        previous_instruction_tokens:
            A list of tokenized versions of all previously generated instructions.
        num_cpus:
            Number of CPUs to use for parallel processing of ROUGE scores.

    Returns:
        A list of seed instructions extracted from the response.
    """
    raw_instructions = re.split(
        pattern=r"###",
        string=f"{num_prompt_instructions + 1}. Instruktion:" + response.completion,
    )
    instructions = []

    for idx, inst in enumerate(raw_instructions, start=1):
        # If the decoding stops due to length, the last example is likely truncated so
        # we discard it
        if idx == len(raw_instructions) and response.done_reason == "length":
            logger.debug(
                "The last instruction was truncated due to length. Skipping it."
            )
            continue

        # If the data does not contain the expected format, we skip it
        splitted_data = re.split(
            pattern=rf"{idx + num_prompt_instructions}\.\s+(Instruktion|Input|Output):",
            string=inst,
        )
        if len(splitted_data) != 7:
            logger.debug(
                f"Skipping instruction {idx} due to unexpected format:\n{inst.strip()}"
            )
            continue

        # Extract instruction, input, and output
        inst = splitted_data[2].strip()
        input = splitted_data[4].strip()
        input = "" if input.lower() == "<noinput>" else input
        output = splitted_data[6].strip()

        # Compute the similarity of the new instruction to all existing
        # instructions
        new_instruction_tokens = scorer._tokenizer.tokenize(text=inst)
        with Pool(processes=num_cpus) as p:
            rouge_scores = p.map(
                partial(rouge_scorer._score_lcs, new_instruction_tokens),
                previous_instruction_tokens,
            )
        rouge_scores = [score.fmeasure for score in rouge_scores]
        most_similar_instructions = {
            previous_instructions[i]: rouge_scores[i]
            for i in np.argsort(rouge_scores)[-10:][::-1]
        }

        # Store the instruction
        new_instruction = InstructionSample(
            instruction=inst,
            input=input,
            output=output,
            most_similar_instructions=most_similar_instructions,
            avg_similarity_score=float(np.mean(rouge_scores)),
            instruction_tokens=new_instruction_tokens,
        )
        instructions.append(new_instruction)

    return instructions
