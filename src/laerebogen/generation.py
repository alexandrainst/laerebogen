"""Generation of the instruction-following data using a language model.

This is the method described in [1], based on [2], and uses seed tasks to generate new
instructions using a decoder model.

[1]: https://github.com/tatsu-lab/stanford_alpaca
[2]: https://doi.org/10.18653/v1/2023.acl-long.754
"""

import json
import logging
import random
from pathlib import Path

from nlp_dedup import Deduper
from pydantic import ValidationError
from tqdm.auto import tqdm

from .data_models import (
    InstructionInput,
    InstructionInputSamples,
    InstructionOutput,
    InstructionSample,
)
from .vllm_utils import generate_text_with_vllm, load_vllm_model

logger = logging.getLogger(__name__)


def generate_instruction_following_data(
    output_dir: Path,
    instruction_generation_prompt_path: Path,
    output_generation_prompt_path: Path,
    seed_tasks_path: Path,
    num_instructions_to_generate: int,
    model_id: str,
    num_prompt_instructions: int,
    batch_size: int,
) -> None:
    """Generate instructions following the seed tasks.

    This function loads seed tasks, loads generated tasks and generates a batch of new
    instructions.

    The new batch is generated by sampling num_prompt_instructions tasks from the seed
    tasks and using them as a prompt for the LLM. The model is asked to generate a new
    instruction task based on the prompt. The generated tasks are filtered based on
    their similarity to the existing tasks and saved to a JSON file.

    Args:
        output_dir:
            Directory to save the generated dataset.
        instruction_generation_prompt_path:
            Path to the prompt file containing the input generation prompt.
        output_generation_prompt_path:
            Path to the prompt file containing the output generation prompt.
        seed_tasks_path:
            Path to the seed tasks file.
        num_instructions_to_generate:
            Number of instructions to generate.
        model_id:
            The model ID of the model to use for generation.
        num_prompt_instructions:
            Number of instructions to use as prompts for each generated instruction.
        batch_size:
            Number of requests to send to the model at once.
    """
    with instruction_generation_prompt_path.open() as f:
        instruction_generation_prompt = f.read()
    with output_generation_prompt_path.open() as f:
        output_generation_prompt = f.read()

    # Load the seed tasks
    with seed_tasks_path.open() as f:
        seed_tasks = [json.loads(line) for line in f.readlines() if line.strip()]
    seed_instruction_data = [
        InstructionSample(
            instruction=t["instruction"] + "\n\n" + t["instances"][0]["input"],
            output=t["instances"][0]["output"],
        )
        for t in seed_tasks
    ]
    logger.info(f"Loaded {len(seed_instruction_data)} human-written seed instructions.")

    # Ensure that the output file exists
    output_path = output_dir / "dataset.jsonl"
    output_path.parent.mkdir(parents=True, exist_ok=True)
    output_path.touch(exist_ok=True)

    # Load the LM-generated instructions
    machine_instruction_data: list[InstructionSample] = list()
    if output_path.exists():
        with output_path.open() as f:
            machine_instruction_data = [
                InstructionSample.model_validate_json(line)
                for line in f.readlines()
                if line.strip()
            ]
        logger.info(
            f"Loaded {len(machine_instruction_data):,} machine-generated instructions."
        )

    if len(machine_instruction_data) >= num_instructions_to_generate:
        return

    model = load_vllm_model(model_id=model_id)

    # Initialise the progress bar
    progress_bar = tqdm(
        total=num_instructions_to_generate,
        initial=len(machine_instruction_data),
        desc="Generating instructions",
    )

    # First we tokenise all the seed instructions and generated machine instructions
    all_instructions = [
        seed_instruction.instruction for seed_instruction in seed_instruction_data
    ] + [instruction.instruction for instruction in machine_instruction_data]

    # Start generating instructions
    while len(machine_instruction_data) < num_instructions_to_generate:
        # Randomly sample some seed instructions, that the new instructions should be
        # based on
        batch_inputs: list[str] = [
            instruction_generation_prompt.format(
                seed_instructions="\n".join(
                    [
                        seed.model_dump_json()
                        for seed in random.sample(
                            population=seed_instruction_data, k=num_prompt_instructions
                        )
                    ]
                )
            ).strip()
            for _ in range(batch_size)
        ]
        responses = generate_text_with_vllm(
            prompts=batch_inputs,
            model=model,
            apply_chat_template=True,
            response_format=InstructionInputSamples,
        )

        # Extract and filter the generated instructions
        instruction_input_data: list[InstructionInput] = list()
        for response in tqdm(
            iterable=responses, desc="Generating new instructions", leave=False
        ):
            if response.done_reason != "stop":
                continue
            try:
                new_instructions = InstructionInputSamples.model_validate_json(
                    response.completion
                ).instructions
            except ValidationError:
                continue
            instruction_input_data.extend(new_instructions)

        # Remove duplicates
        deduper = Deduper(
            store_mask_to_disk=False,
            store_config_to_disk=False,
            store_corpus_to_disk=False,
            store_lsh_cache_to_disk=False,
            return_generator=True,
            similarity_threshold=0.8,  # The amount of overlap to count as a duplicate
            verbose=False,
        )
        instruction_input_data = [
            instruction_input_data[mask["id"] - len(all_instructions)]
            for mask in tqdm(
                iterable=deduper.deduplicate(
                    corpus=(
                        all_instructions  # type: ignore[bad-argument-type]
                        + [obj.instruction for obj in instruction_input_data]
                    ),
                    overwrite=True,
                )
                or [],
                desc="Removing duplicate instructions",
                leave=False,
                total=len(instruction_input_data),
            )
            if mask["id"] >= len(all_instructions) and not mask["duplicate"]
        ]

        if instruction_input_data:
            # Generate outputs for the deduplicated instructions
            prompts = [
                output_generation_prompt.format(instruction=instruction.instruction)
                for instruction in instruction_input_data
            ]
            responses = generate_text_with_vllm(
                prompts=prompts,
                model=model,
                apply_chat_template=True,
                response_format=InstructionOutput,
            )
            assert len(instruction_input_data) == len(responses), (
                f"Number of responses ({len(responses)}) does not match number of "
                f"input instructions ({len(instruction_input_data)})."
            )

            # Extract and filter the generated outputs
            instruction_data: list[InstructionSample] = list()
            for instruction, response in zip(instruction_input_data, responses):
                if response.done_reason != "stop":
                    continue
                try:
                    new_output = InstructionOutput.model_validate_json(
                        response.completion
                    ).output
                except ValidationError:
                    continue
                instruction_sample = InstructionSample(
                    instruction=instruction.instruction, output=new_output
                )
                instruction_data.append(instruction_sample)

            # Update the existing intermediate data
            machine_instruction_data.extend(instruction_data)
            all_instructions.extend(
                instruction.instruction for instruction in instruction_data
            )

            # Update the progress bar
            progress_bar.update(len(instruction_data))

            # Store the generated instructions to disk
            with output_path.open("a") as f:
                json_records = "\n".join(
                    instruction.model_dump_json() for instruction in instruction_data
                )
                f.write(json_records + "\n")

    progress_bar.close()
