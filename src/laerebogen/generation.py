"""Generation of the instruction-following data using a language model.

This is the method described in [1], based on [2], and uses seed tasks to generate new
instructions using a decoder model.

[1]: https://github.com/tatsu-lab/stanford_alpaca
[2]: https://doi.org/10.18653/v1/2023.acl-long.754
"""

import json
import logging
import random
from copy import deepcopy
from pathlib import Path

from nlp_dedup import Deduper
from pydantic import ValidationError
from tqdm.auto import tqdm

from .data_models import InstructionSample, InstructionSamples, Response
from .filtering import keep_instruction
from .vllm_utils import generate_text_with_vllm, load_vllm_model

logger = logging.getLogger(__name__)


def generate_instruction_following_data(
    output_dir: str,
    prompt_path: str,
    seed_tasks_path: str,
    num_instructions_to_generate: int,
    model_id: str,
    num_prompt_instructions: int,
    batch_size: int,
) -> None:
    """Generate instructions following the seed tasks.

    This function loads seed tasks, loads generated tasks and generates a batch of new
    instructions.

    The new batch is generated by sampling num_prompt_instructions tasks from the seed
    tasks and using them as a prompt for the LLM. The model is asked to generate a new
    instruction task based on the prompt. The generated tasks are filtered based on
    their similarity to the existing tasks and saved to a JSON file.

    Args:
        output_dir:
            Directory to save the generated dataset.
        prompt_path:
            Path to the prompt file.
        seed_tasks_path:
            Path to the seed tasks file.
        num_instructions_to_generate:
            Number of instructions to generate.
        model_id:
            The model ID of the model to use for generation.
        num_prompt_instructions:
            Number of instructions to use as prompts for each generated instruction.
        batch_size:
            Number of requests to send to the model at once.
    """
    logger.info(f"Loading model {model_id!r} for generating instructions...")
    model = load_vllm_model(model_id=model_id)

    # Load the prompt
    with Path(prompt_path).open() as f:
        generation_prompt = f.read()

    # Load the seed tasks
    with Path(seed_tasks_path).open() as f:
        seed_tasks = [json.loads(line) for line in f.readlines() if line.strip()]
    seed_instruction_data = [
        InstructionSample(
            instruction=t["instruction"] + "\n\n" + t["instances"][0]["input"],
            output=t["instances"][0]["output"],
        )
        for t in seed_tasks
    ]
    logger.info(f"Loaded {len(seed_instruction_data)} human-written seed instructions.")

    # Ensure that the output file exists
    output_path = Path(output_dir, "dataset.jsonl")
    output_path.parent.mkdir(parents=True, exist_ok=True)
    output_path.touch(exist_ok=True)

    # Load the LM-generated instructions
    machine_instruction_data = []
    if output_path.exists():
        with output_path.open() as f:
            machine_instruction_data = [
                json.loads(line) for line in f.readlines() if line.strip()
            ]
        logger.info(
            f"Loaded {len(machine_instruction_data):,} machine-generated instructions."
        )

    # Initialise the progress bar
    progress_bar = tqdm(
        total=num_instructions_to_generate,
        initial=len(machine_instruction_data),
        desc="Generating instructions",
    )

    # First we tokenise all the seed instructions and generated machine instructions
    all_instructions = [
        seed_instruction.instruction for seed_instruction in seed_instruction_data
    ] + [instruction["instruction"] for instruction in machine_instruction_data]

    # Start generating instructions
    while len(machine_instruction_data) < num_instructions_to_generate:
        # Randomly sample some seed instructions, that the new instructions should be
        # based on
        batch_inputs = []
        for _ in range(batch_size):
            seed_instructions = random.sample(
                population=seed_instruction_data, k=num_prompt_instructions
            )
            encoded_prompt = encode_prompt(
                seed_instructions=seed_instructions, prompt=generation_prompt
            )
            batch_inputs.append(encoded_prompt)

        # Generate new instructions with the LLM
        responses = generate_text_with_vllm(
            prompts=batch_inputs, model=model, response_format=InstructionSamples
        )

        # Process the generated instructions
        instruction_data: list[InstructionSample] = []
        for response in tqdm(
            iterable=responses, desc="Processing responses", leave=False
        ):
            new_instructions = post_process_response(
                response=response, previous_instructions=all_instructions
            )
            for instruction in new_instructions:
                if keep_instruction(instruction_sample=instruction):
                    instruction_data.append(instruction)

        if instruction_data:
            # Update the existing intermediate data
            machine_instruction_data.extend(instruction_data)
            all_instructions.extend(
                instruction.instruction for instruction in instruction_data
            )

            # Update the progress bar
            progress_bar.update(len(instruction_data))

            # Store the generated instructions to disk
            with output_path.open("a") as f:
                json_records = "\n".join(
                    instruction.json() for instruction in instruction_data
                )
                f.write(json_records + "\n")

    # Close the progress bar
    progress_bar.close()


def encode_prompt(seed_instructions: list[InstructionSample], prompt: str) -> str:
    """Encode multiple prompt instructions into a single string.

    Args:
        seed_instructions:
            A list of seed instructions to encode into the prompt.
        prompt:
            The initial prompt text to which the seed instructions will be appended.

    Returns:
        A string containing the encoded prompt.
    """
    encoded_prompt = (
        deepcopy(prompt)
        .strip()
        .format(
            seed_instructions="\n".join(
                [
                    json.dumps(dict(instruction=seed.instruction, output=seed.output))
                    for seed in seed_instructions
                ]
            )
        )
    )
    return encoded_prompt.strip()


def post_process_response(
    response: Response, previous_instructions: list[str]
) -> list[InstructionSample]:
    """Post-process the response from the model to extract instructions.

    Args:
        response:
            The response from the model.
        previous_instructions:
            A list of all previously generated instructions.

    Returns:
        A list of instructions extracted from the response.
    """
    # Copy previous_instructions to avoid modifying the original lists
    previous_instructions = deepcopy(previous_instructions)

    try:
        instruction_objects = InstructionSamples.model_validate_json(
            response.completion
        ).instructions
    except ValidationError:
        logger.warning(
            "Failed to parse the response as a list of instructions. Skipping it."
        )
        return []

    # Remove duplicates
    deduper = Deduper(
        store_mask_to_disk=False,
        store_config_to_disk=False,
        store_corpus_to_disk=False,
        store_lsh_cache_to_disk=False,
        return_generator=True,
    )
    instruction_objects = [
        instruction_objects[mask["id"] - len(previous_instructions)]
        for mask in tqdm(
            deduper.deduplicate(
                corpus=(
                    previous_instructions  # Â type: ignore[bad-argument-type]
                    + [obj.instruction for obj in instruction_objects]
                ),
                overwrite=True,
            )
            or [],
            desc="Removing duplicates",
            leave=False,
            total=len(instruction_objects),
        )
        if mask["id"] >= len(previous_instructions) and not mask["duplicate"]
    ]

    return instruction_objects
